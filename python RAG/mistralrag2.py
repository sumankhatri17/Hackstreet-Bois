# -*- coding: utf-8 -*-
"""MistralRAG2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A7t3CZMvF8n6rcgN6uIgMykpcJhw_uyR
"""

!pip install mistralai
!pip install faiss-cpu

import os
from mistralai import Mistral
import requests
import numpy as np
import faiss
import os
import json
import uuid
import re
from pathlib import Path
from getpass import getpass
from google.colab import userdata
api_key= userdata.get("MISTRAL_API_KEY")
client = Mistral(api_key=api_key)

all_objects = []
per_file_objects = {}
folder_path = "/content/JSON_DOCS"
for filename in os.listdir(folder_path):
    file_path = os.path.join(folder_path, filename)
    if os.path.isfile(file_path) and filename.lower().endswith((".jsonl", ".ndjson")):
        with open(file_path, "r", encoding="utf-8") as f:
            objs = []
            for line_no, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    objs.append(obj)
                    all_objects.append(obj)
                except json.JSONDecodeError:
                    print(f"Warning: skipping invalid JSON in {filename} at line {line_no}")
            per_file_objects[filename] = objs

folder_path = "/content/JSON_DOCS"   # directory containing .jsonl/.ndjson
chunk_size = 2048
chunk_overlap = 200
embed_model = "mistral-embed"
chat_model = "mistral-large-latest"
batch_size = 32
out_dir = Path("./faiss_index_out")
index_filename = "faiss_index.bin"
metadata_filename = "metadata.jsonl"
use_l2 = True  # True => IndexFlatL2; False => IndexFlatIP
normalize_for_cosine = False  # only meaningful if using IP for cosine sim
top_k = 5

dummy_essay_content = """The history of artificial intelligence (AI) dates back to ancient myths and philosophical inquiries about intelligent beings, but the modern field of AI was established in 1956 at a workshop at Dartmouth College. Pioneers like John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon are often credited with coining the term 'artificial intelligence' and initiating research into machine learning, natural language processing, and problem-solving. Early AI research focused on symbolic methods, attempting to encode human-like reasoning into machines using rules and logic. Projects like the General Problem Solver aimed to solve a wide range of problems using a unified approach.

However, the initial optimism of AI's early days faced significant challenges, leading to what is known as the 'AI winter' in the 1980s. Limitations in computational power, data availability, and the complexity of real-world problems proved difficult to overcome with symbolic AI. Despite these setbacks, research continued, particularly in areas like expert systems, which saw commercial success in specialized domains. The advent of faster computers, larger datasets, and new algorithms, particularly in neural networks and deep learning, revitalized the field in the early 21st century.

Today, AI is integrated into countless aspects of daily life, from recommendation systems and autonomous vehicles to medical diagnostics and scientific research. Machine learning, a subfield of AI, allows systems to learn from data without explicit programming. Deep learning, a subset of machine learning, utilizes neural networks with many layers to model complex patterns, leading to breakthroughs in image recognition, speech processing, and game playing. The future of AI promises further advancements, raising profound questions about ethics, employment, and the nature of intelligence itself. As AI continues to evolve, its impact on society will undoubtedly be one of the most significant technological developments of our time."""

text = dummy_essay_content

print(f"Length of dummy essay content: {len(text)}")

chunk_size = 2048
chunk_overlap = 200


_SENT_SPLIT_RE = re.compile(r'(?<=[\.\?\!])\s+')

def split_into_sentences(text: str):
    parts = _SENT_SPLIT_RE.split(text.strip())
    return [p.strip() for p in parts if p.strip()]

def chunk_text(text: str, chunk_size: int = 2048, overlap: int = 200):
    if not text:
        return []
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    sentences = split_into_sentences(text)
    chunks = []
    current = []
    current_len = 0
    for sent in sentences:
        s_len = len(sent) + 1
        if current_len + s_len <= chunk_size or not current:
            current.append(sent)
            current_len += s_len
        else:
            chunks.append(" ".join(current).strip())
            if overlap > 0:
                overlap_chars = 0
                overlap_sents = []
                for prev_sent in reversed(current):
                    overlap_chars += len(prev_sent) + 1
                    overlap_sents.insert(0, prev_sent)
                    if overlap_chars >= overlap:
                        break
                current = overlap_sents.copy()
                current_len = sum(len(x) + 1 for x in current)
            else:
                current = []
                current_len = 0
            if not current:
                current.append(sent)
                current_len = s_len
            elif current_len + s_len <= chunk_size:
                current.append(sent)
                current_len += s_len
            else:
                part_start = 0
                while part_start < len(sent):
                    part = sent[part_start: part_start + chunk_size]
                    chunks.append(part.strip())
                    part_start += chunk_size - overlap if overlap < chunk_size else chunk_size
                current = []
                current_len = 0
    if current:
        chunks.append(" ".join(current).strip())
    return chunks

all_chunks = []

for obj in all_objects:
    text = obj.get("text", "") or obj.get("content", "") or ""
    if not isinstance(text, str) or not text.strip():
        continue
    src_id = obj.get("id")
    src_file = obj.get("file")
    section = obj.get("section_label")
    chunk_texts = chunk_text(text, chunk_size=chunk_size, overlap=chunk_overlap)
    for idx, ctext in enumerate(chunk_texts):
        chunk_doc = {
            "chunk_id": str(uuid.uuid4()),
            "source_id": src_id,
            "source_file": src_file,
            "section_label": section,
            "chunk_index": idx,
            "text": ctext,
            "orig_length": len(text),
        }
        all_chunks.append(chunk_doc)

import numpy as np
from typing import List, Dict, Tuple, Any, Optional

def generate_embeddings_for_chunks(
    all_chunks: List[Dict[str, Any]],
    client: Any,
    model: str = "mistral-embed",
    batch_size: int = 32,
    show_progress: bool = True,
) -> Tuple[np.ndarray, List[Dict[str, Any]]]:
    """
    Generate embeddings for `all_chunks` using `client.embeddings.create`.

    Args:
      all_chunks: list of dicts, each must contain at least the key "text".
      client: an API client with `client.embeddings.create(model=..., inputs=[...])`.
              Response is expected to have a `.data` attribute or be a dict with "data".
              Each data item must expose an `embedding` list (either dict["embedding"] or .embedding).
      model: embedding model name (default "mistral-embed").
      batch_size: number of texts per API call.
      show_progress: if True uses a simple counter print to indicate progress.

    Returns:
      Tuple of:
        - embeddings: np.ndarray of shape (N, D) dtype float32
        - metadata_list: list of metadata dicts aligned with embeddings by index
          each metadata dict contains at least: embedding_index, chunk_id, source_id, source_file,
          section_label, chunk_index, orig_length

    Raises:
      RuntimeError on unexpected API responses.
    """
    if not isinstance(all_chunks, list) or not all_chunks:
        return np.zeros((0, 0), dtype=np.float32), []

    # Extract texts and validate items
    texts = []
    for i, c in enumerate(all_chunks):
        if not isinstance(c, dict):
            raise RuntimeError(f"all_chunks element at index {i} is not a dict")
        t = c.get("text") or c.get("content") or ""
        if not isinstance(t, str):
            t = str(t)
        texts.append(t)

    embeddings_list: List[List[float]] = []
    total = len(texts)
    if show_progress:
        print(f"Embedding {total} chunks in batches of {batch_size} (model={model})...")

    for start in range(0, total, batch_size):
        batch_texts = texts[start : start + batch_size]

        # Call embeddings API
        resp = client.embeddings.create(model=model, inputs=batch_texts)

        # Robustly extract data items
        if isinstance(resp, dict):
            data_items = resp.get("data")
        else:
            data_items = getattr(resp, "data", None)

        if data_items is None:
            raise RuntimeError("Embedding response did not contain `data`")

        if len(data_items) != len(batch_texts):
            raise RuntimeError(
                f"Embedding response length {len(data_items)} != batch size {len(batch_texts)}"
            )

        # Extract embedding vectors in order
        for item in data_items:
            if isinstance(item, dict):
                emb = item.get("embedding")
            else:
                emb = getattr(item, "embedding", None)
            if emb is None:
                raise RuntimeError("Embedding item missing `embedding` field")
            embeddings_list.append(emb)

        if show_progress:
            done = min(start + batch_size, total)
            print(f"  embedded {done}/{total}")

    # Convert to numpy array
    emb_array = np.array(embeddings_list, dtype=np.float32)
    # Align metadata with embeddings
    metadata_list: List[Dict[str, Any]] = []
    for idx, chunk in enumerate(all_chunks):
        metadata = {
            "embedding_index": idx,
            "chunk_id": chunk.get("chunk_id"),
            "source_id": chunk.get("source_id"),
            "source_file": chunk.get("source_file"),
            "section_label": chunk.get("section_label"),
            "chunk_index": chunk.get("chunk_index"),
            "orig_length": chunk.get("orig_length"),
        }
        metadata_list.append(metadata)

    return emb_array, metadata_list

def build_and_save_faiss_index(
    emb_array: np.ndarray,
    metadata_list: List[Dict[str, Any]],
    out_dir: str = "./faiss_index_out",
    use_l2: bool = True,
    normalize_for_cosine: bool = False,
    index_filename: str = "faiss_index.bin",
    metadata_filename: str = "metadata.jsonl",
) -> Dict[str, str]:
    """
    Build a FAISS index from `emb_array`, save the index and metadata.

    Args:
      emb_array: numpy array of shape (N, D) dtype float32 (or convertible).
      metadata_list: list of metadata dicts aligned with emb_array rows.
      out_dir: directory to write the FAISS index and metadata.
      use_l2: if True use IndexFlatL2, otherwise use IndexFlatIP (inner product).
      normalize_for_cosine: if True, L2-normalize vectors before indexing (useful when using IP for cosine).
      index_filename: filename for saved index (under out_dir).
      metadata_filename: filename for saved metadata jsonl (under out_dir).

    Returns:
      dict with keys: index_path, metadata_path, ntotal, dim
    """
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)

    # Convert to numpy float32
    arr = np.asarray(emb_array)
    if arr.dtype != np.float32:
        arr = arr.astype("float32")

    if arr.ndim != 2:
        raise ValueError(f"emb_array must be 2D (N, D); got shape {arr.shape}")

    n, d = arr.shape

    # Optional normalization (in-place copy)
    if normalize_for_cosine:
        # normalize L2 so we can use inner product as cosine similarity
        faiss.normalize_L2(arr)

    # Choose index type
    if use_l2:
        index = faiss.IndexFlatL2(d)
    else:
        index = faiss.IndexFlatIP(d)

    # Add vectors
    index.add(arr)
    print(f"FAISS index built with {index.ntotal} vectors of dimension {d} (use_l2={use_l2}, normalized={normalize_for_cosine}).")

    # Save index
    index_path = out_path / index_filename
    faiss.write_index(index, str(index_path))
    print(f"Saved FAISS index to: {index_path}")

    # Save metadata aligned to embedding rows
    meta_path = out_path / metadata_filename
    if len(metadata_list) != n:
        print(f"Warning: metadata_list length ({len(metadata_list)}) != number of vectors ({n}). Truncating/padding as necessary.")
    with meta_path.open("w", encoding="utf8") as mf:
        for i in range(n):
            meta = metadata_list[i] if i < len(metadata_list) else {}
            # Ensure embedding_index present
            if "embedding_index" not in meta:
                meta["embedding_index"] = i
            mf.write(json.dumps(meta, ensure_ascii=False) + "\n")
    print(f"Saved metadata JSONL to: {meta_path}")

    return {
        "index_path": str(index_path),
        "metadata_path": str(meta_path),
        "ntotal": int(index.ntotal),
        "dim": int(d),
    }

def retrieve_top_k_chunks(
    question: str,
    client: Any,
    index: Optional[faiss.Index] = None,
    index_path: Optional[str] = None,
    metadata_path: Optional[str] = None,
    all_chunks: Optional[List[Dict[str, Any]]] = None,
    model: str = "mistral-embed",
    k: int = 1,
    normalize_query: bool = False,
) -> Dict[str, Any]:
    """
    Embed a question, run a FAISS search, and return the top-k retrieved chunks.

    Args:
      question: query string to embed and search with.
      client: API client that supports `client.embeddings.create(model=..., inputs=[...])`.
      index: optional in-memory faiss.Index object. If not provided, index_path must be given.
      index_path: path to faiss index file (used if `index` is None).
      metadata_path: path to metadata JSONL aligned with indexed vectors (required if `all_chunks` not provided).
                     Each line should be a JSON object with at least embedding_index and optionally chunk text/metadata.
      all_chunks: optional list of chunk dicts produced during chunking (must align with index row order).
                  If provided, retrieved chunk texts will be taken from here; otherwise metadata_path must contain text.
      model: embedding model name used for creating the question embedding (default "mistral-embed").
      k: number of nearest neighbors to retrieve.
      normalize_query: if True, L2-normalize the query vector before search (useful if index built on normalized vectors).

    Returns:
      A dict with keys:
        - "question_embedding": numpy array shape (1, D)
        - "D": distances array returned by faiss
        - "I": indices array returned by faiss
        - "retrieved": list of retrieved chunk dicts (each contains index, score, text, metadata)
    Raises:
      RuntimeError on unexpected API response or missing required inputs.
    """

    # 1) Obtain or load FAISS index
    if index is None:
        if not index_path:
            raise RuntimeError("Either a faiss.Index object or index_path must be provided.")
        index = faiss.read_index(index_path)

    # 2) Create question embedding via client
    resp = client.embeddings.create(model=model, inputs=[question])

    # Robust extraction of response.data
    if isinstance(resp, dict):
        data_items = resp.get("data")
    else:
        data_items = getattr(resp, "data", None)

    if not data_items or len(data_items) < 1:
        raise RuntimeError("Embedding API response did not contain expected `data` for the question embedding.")

    first_item = data_items[0]
    if isinstance(first_item, dict):
        q_emb = first_item.get("embedding")
    else:
        q_emb = getattr(first_item, "embedding", None)

    if q_emb is None:
        raise RuntimeError("Embedding item missing `embedding` field for the question.")

    # Convert to numpy (1, D)
    qvec = np.array([q_emb], dtype=np.float32)

    if normalize_query:
        faiss.normalize_L2(qvec)

    # 3) Run search
    D, I = index.search(qvec, k)  # D: distances, I: indices

    # 4) Load metadata if needed
    metadata_list: List[Dict[str, Any]] = []
    if all_chunks is None:
        if not metadata_path:
            raise RuntimeError("metadata_path must be provided if all_chunks is not supplied.")
        with open(metadata_path, "r", encoding="utf8") as mf:
            for line in mf:
                line = line.strip()
                if not line:
                    continue
                try:
                    metadata_list.append(json.loads(line))
                except json.JSONDecodeError:
                    # skip malformed lines
                    metadata_list.append({})

    # 5) Build retrieved_chunk list similar to user's example
    retrieved_chunks: List[Dict[str, Any]] = []
    idxs = I.tolist()[0]
    dists = D.tolist()[0]
    for rank, idx in enumerate(idxs):
        if idx == -1:
            continue
        score = dists[rank]
        # Prefer all_chunks for text if provided
        chunk_text = None
        chunk_meta: Dict[str, Any] = {}
        if all_chunks is not None:
            if 0 <= idx < len(all_chunks):
                chunk_item = all_chunks[idx]
                chunk_text = chunk_item.get("text")
                chunk_meta = {k: v for k, v in chunk_item.items() if k != "text"}
        else:
            # try to get text from metadata JSONL
            if 0 <= idx < len(metadata_list):
                meta_item = metadata_list[idx]
                chunk_text = meta_item.get("text") or meta_item.get("chunk_text") or None
                chunk_meta = meta_item

        retrieved_chunks.append(
            {
                "rank": rank,
                "index": idx,
                "score": float(score),
                "text": chunk_text,
                "metadata": chunk_meta,
            }
        )

    # 6) Print summary similar to user's example
    print(f"Question embedding shape: {qvec.shape}")
    print(f"Retrieved distances (D):\n{D}")
    print(f"Retrieved indices (I):\n{I}")
    print(f"Number of retrieved chunks: {len(retrieved_chunks)}")
    if retrieved_chunks:
        first_text = retrieved_chunks[0].get("text") or ""
        print(f"First retrieved chunk: {first_text[:200]}...")
    else:
        print("No chunks retrieved.")

    return {
        "question_embedding": qvec,
        "D": D,
        "I": I,
        "retrieved": retrieved_chunks,
    }

index_path = out_dir / index_filename
metadata_path = out_dir / metadata_filename

index_obj: Optional[faiss.Index] = None
index_exists = index_path.exists() and metadata_path.exists()

if index_exists:
    try:
        print(f"Loading existing FAISS index from {index_path} ...")
        index_obj = faiss.read_index(str(index_path))
        print("Index loaded.")
    except Exception as e:
        print(f"Failed to load existing index ({e}). Will rebuild from embeddings.")

if index_obj is None:
    if not all_chunks:
        print("No chunks to index. Skipping index build.")
    else:
        # Create embeddings and build index
        emb_array, metadata_list = generate_embeddings_for_chunks(
            all_chunks=all_chunks,
            client=client,
            model=embed_model,
            batch_size=batch_size,
            show_progress=True,
        )
        idx_info = build_and_save_faiss_index(
            emb_array=emb_array,
            metadata_list=metadata_list,
            out_dir=out_dir,
            use_l2=use_l2,
            normalize_for_cosine=normalize_for_cosine,
            index_filename=index_filename,
            metadata_filename=metadata_filename,
        )
        index_obj = faiss.read_index(idx_info["index_path"])
        print(f"Index built and loaded: {idx_info}")

# ---- Interactive question loop (suitable for Colab) ----
print("\nRAG pipeline ready. Enter a question to retrieve and answer (empty input to exit).")
while True:
    try:
        question = input("\nQuestion: ").strip()
    except Exception:
        # In some notebook contexts input() may not work; fallback to single run
        print("input() not available. Exiting interactive loop.")
        break
    if not question:
        print("Exiting.")
        break

    # Use index_obj if available, otherwise try loading from disk or skip retrieval
    if index_obj is None:
        print("No FAISS index available. Running Mistral model without retrieved context.")
        retrieved = []
    else:
        try:
            retrieval_result = retrieve_top_k_chunks(
                question=question,
                client=client,
                index=index_obj,
                all_chunks=all_chunks if all_chunks else None,
                metadata_path=str(metadata_path) if metadata_path.exists() else None,
                model=embed_model,
                k=top_k,
                normalize_query=normalize_for_cosine,
            )
            retrieved = retrieval_result.get("retrieved", [])
        except Exception as e:
            print(f"Retrieval failed: {e}")
            retrieved = []

    # Build context for the chat model from retrieved chunks
    context_pieces: List[str] = []
    for rc in retrieved:
        meta = rc.get("metadata", {})
        src = meta.get("source_file") or meta.get("source_id") or meta.get("chunk_id") or "unknown"
        idx = meta.get("chunk_index") if isinstance(meta.get("chunk_index"), int) else rc.get("index")
        header = f"[source: {src} index:{idx} score:{rc.get('score'):.4f}]"
        text = rc.get("text") or meta.get("text") or ""
        context_pieces.append(f"{header}\n{text}")

    context_text = "\n\n---\n\n".join(context_pieces) if context_pieces else "No retrieved context available."

    prompt = (
        "You are a helpful assistant. Use ONLY the context provided below to answer the user's question. "
        "Do not use any outside knowledge beyond the context. If the answer is not contained in the context, say you don't know.\n\n"
        "Context:\n"
        f"{context_text}\n\n"
        f"Question: {question}\n\nAnswer:"
    )

    print("\n===== Prompt sent to chat model (truncated) =====")
    print(prompt[:2000] + ("\n... (truncated)" if len(prompt) > 2000 else ""))
    print("===============================================")

    try:
        # Use the chat client you had in your snippet. Adjust to the client's actual response shape if needed.
        messages = [{"role": "user", "content": prompt}]
        chat_response = client.chat.complete(model=chat_model, messages=messages)
        # Try to robustly extract content
        answer = None
        if isinstance(chat_response, dict):
            # common dict shapes
            choices = chat_response.get("choices")
            if choices and isinstance(choices, list) and len(choices) > 0:
                c0 = choices[0]
                # try nested fields
                if isinstance(c0, dict):
                    # many clients: c0["message"]["content"] or c0["text"]
                    msg = c0.get("message")
                    if isinstance(msg, dict):
                        answer = msg.get("content")
                    else:
                        answer = c0.get("text") or c0.get("content")
        else:
            # object-like response
            try:
                answer = chat_response.choices[0].message.content
            except Exception:
                try:
                    answer = chat_response.choices[0].text
                except Exception:
                    answer = None

        if not answer:
            print("Warning: Could not extract answer from chat response. Full response:")
            print(chat_response)
        else:
            print("\n===== Model Answer =====")
            print(answer)
            print("========================")
    except Exception as e:
        print(f"Chat model call failed: {e}")
        # optionally show full exception stack in Colab
        import traceback
        traceback.print_exc()

prompt = f"""
Context information is below.
---------------------
{retrieved_chunk}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
"""

print(prompt)

def run_mistral(user_message, model="mistral-large-latest"):
    messages = [
        {
            "role": "user", "content": user_message
        }
    ]
    chat_response = client.chat.complete(
        model=model,
        messages=messages
    )
    return (chat_response.choices[0].message.content)

run_mistral(prompt)